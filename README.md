# Building-n-layers-neural-network-from-scartch

ğŸ§  N-Layer Neural Network from Scratch (NumPy)

This repository contains a fully vectorized implementation of an N-layer (deep) neural network built from scratch using NumPy only, without relying on deep learning frameworks such as TensorFlow, PyTorch, or Keras.

The code is beginner friendly so every one can understand and benifit from it, only you need is a math

The project generalizes a 2-layer neural network to arbitrary depth, demonstrating a clear understanding of forward propagation, backpropagation, and gradient descent across multiple layers.

ğŸ“Œ Model Architecture (Generalized)
Input Layer
    â†“
Hidden Layer 1 (ReLU)
    â†“
Hidden Layer 2 (ReLU)
    â†“
...
    â†“
Hidden Layer Lâˆ’1 (ReLU)
    â†“
Output Layer (Softmax)


Example configuration:

layer_dims = [784, 64, 32, 10]


Which corresponds to:

784 â†’ 64 â†’ 32 â†’ 10

ğŸ§® Mathematical Formulation

For an N-layer network:

Forward Propagation (Layer l)
ğ‘
(
ğ‘™
)
=
ğ‘Š
(
ğ‘™
)
ğ´
(
ğ‘™
âˆ’
1
)
+
ğ‘
(
ğ‘™
)
Z
(l)
=W
(l)
A
(lâˆ’1)
+b
(l)
ğ´
(
ğ‘™
)
=
{
ReLU
(
ğ‘
(
ğ‘™
)
)
	
ğ‘™
<
ğ¿


Softmax
(
ğ‘
(
ğ‘™
)
)
	
ğ‘™
=
ğ¿
A
(l)
={
ReLU(Z
(l)
)
Softmax(Z
(l)
)
	â€‹

l<L
l=L
	â€‹

Loss Function

Categorical Cross-Entropy (Log Loss)

ğ¿
=
âˆ’
1
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
âˆ‘
ğ‘
=
1
ğ¶
ğ‘¦
ğ‘
(
ğ‘–
)
log
â¡
(
ğ‘¦
^
ğ‘
(
ğ‘–
)
)
L=âˆ’
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

c=1
âˆ‘
C
	â€‹

y
c
(i)
	â€‹

log(
y
^
	â€‹

c
(i)
	â€‹

)
Backpropagation (General Case)

Starting from the output layer:

ğ‘‘
ğ‘
(
ğ¿
)
=
ğ´
(
ğ¿
)
âˆ’
ğ‘Œ
dZ
(L)
=A
(L)
âˆ’Y

For each layer 
ğ‘™
=
ğ¿
,
ğ¿
âˆ’
1
,
.
.
.
,
1
l=L,Lâˆ’1,...,1:

ğ‘‘
ğ‘Š
(
ğ‘™
)
=
1
ğ‘š
ğ‘‘
ğ‘
(
ğ‘™
)
ğ´
(
ğ‘™
âˆ’
1
)
ğ‘‡
dW
(l)
=
m
1
	â€‹

dZ
(l)
A
(lâˆ’1)T
ğ‘‘
ğ‘
(
ğ‘™
)
=
1
ğ‘š
âˆ‘
ğ‘‘
ğ‘
(
ğ‘™
)
db
(l)
=
m
1
	â€‹

âˆ‘dZ
(l)
ğ‘‘
ğ‘
(
ğ‘™
âˆ’
1
)
=
ğ‘Š
(
ğ‘™
)
ğ‘‡
ğ‘‘
ğ‘
(
ğ‘™
)
âŠ™
ReLU
â€²
(
ğ‘
(
ğ‘™
âˆ’
1
)
)
dZ
(lâˆ’1)
=W
(l)T
dZ
(l)
âŠ™ReLU
â€²
(Z
(lâˆ’1)
)
ğŸ› ï¸ Implementation Details

Language: Python

Libraries: NumPy only

No ML/DL frameworks used

Fully vectorized operations

Parameters stored using Python lists for scalability

Clean separation of:

initialization

forward propagation

backward propagation

parameter updates

ğŸ“‚ Code Structure
.
â”œâ”€â”€ init_params(layer_dims)     # Initialize weights & biases for N layers
â”œâ”€â”€ forward_prop(X, Ws, bs)     # Forward propagation through all layers
â”œâ”€â”€ backward_prop(Zs, As, Ws)   # Backpropagation across N layers
â”œâ”€â”€ update_params(Ws, bs)       # Gradient descent update
â”œâ”€â”€ one_hot(Y)                  # Label encoding
â””â”€â”€ training loop

ğŸš€ Training Loop (High Level)
Ws, bs = init_params(layer_dims)

for epoch in range(epochs):
    Zs, As = forward_prop(X_train, Ws, bs)
    dWs, dbs = backward_prop(Zs, As, Ws, X_train, Y_train)
    Ws, bs = update_params(Ws, bs, dWs, dbs, learning_rate)

ğŸ¯ Key Learning Outcomes

How deep networks generalize shallow networks

Why lists + loops scale better than hard-coded layers

How gradient flow works across many layers

Why non-linearity is essential for depth

How vectorized backpropagation is implemented for arbitrary depth

âš ï¸ Notes

This is an educational implementation, not production-optimized

Initialization uses simple random values (no Xavier / He)

No regularization or batch normalization included

Designed for clarity over performance

ğŸ“Œ Why This Project?

Most tutorials stop at 1â€“2 layers or hide complexity behind frameworks.
This project demonstrates true understanding of deep learning mechanics by scaling neural networks to any number of layers using only linear algebra.

ğŸ”œ Possible Extensions

Xavier / He initialization

Dropout

Batch normalization

Accuracy & loss tracking

Modular activation functions

ğŸ‘¤ Author

Abraar
GitHub: Abraar77
